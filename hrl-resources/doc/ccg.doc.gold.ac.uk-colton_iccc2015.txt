The Painting Fool Sees! New Projects with the Automated Painter
Simon Colton1,2 , Jakob Halskov3 , Dan Ventura4 ,
Ian Gouldstone2 , Michael Cook2 and Blanca P´erez-Ferrer1
1

2

The MetaMakers Institute, Academy for Innovation and Research, Falmouth University, UK
Computational Creativity Group, Department of Computing, Goldsmiths, University of London, UK
3
UBIC Inc., Tokyo, Japan
4
Computer Science Department, Brigham Young University, USA
Abstract

We report the most recent advances in The Painting Fool
project, where we have integrated machine vision capabilities from the DARCI system into the automated
painter, to enhance its abilities before, during and after
the painting process. This has enabled new art projects,
including a commission from an Artificial Intelligence
company, and we report on this collaboration, which is
one of the first instances in Computational Creativity research where creative software has been commissioned
directly. The new projects have advanced The Painting
Fool as an independent artist able to produce more diverse styles which break away from simulating natural
media. The projects have also raised a philosophical
question about whether software artists need to see in
the same way as people, which we discuss briefly.

Introduction
The Painting Fool (thepaintingfool.com) is software
that we hope will be taken seriously as a creative artist in
its own right, one day. It is a well established project,
with an emphasis on implementing processes which could
be described as artistic and/or creative, rather than merely
producing images which look like they may have been
painted by a person, as with many graphics packages, as per
(Strothotte and Schlechtweg 2002). Many technical details
of the project and discussions of the outreach activities performed with The Painting Fool are given in (Colton 2012b).
Progress in the project is usually both technical and/or societal, and the work presented here addresses both aspects.
On the technical side, we have enabled The Painting Fool
to use machine vision techniques before, during and after
painting, to take on more creative responsibility, produce
more interesting pieces and provide better framing information. This has involved integrating aspects of the machine
vision abilities of the DARCI system (Norton, Heath, and
Ventura 2013; Heath, Norton, and Ventura 2014). In addition to being used in art generation itself (Norton, Heath,
and Ventura 2011), DARCI has been used as an artificial art
critic (Norton, Heath, and Ventura 2010), which makes it
the perfect complement to The Painting Fool. Implementing
such synergies is rare in Computational Creativity research,
with a few notable exceptions, such as the combination of
parts of the MEXICA, Curveship and GRIOT programs into
the Slant storytelling system (Montfort et al. 2013).

On the societal side, to get The Painting Fool accepted as
an artist, we engage the public, journalists and members of
the art world (artists, art students, art educators, critics, curators, gallery owners, etc.), as natural stakeholders in the
question of whether software can be creative or not. Exploration of some of the stakeholders issues in Computational
Creativity is given in (Colton et al. 2015), where The Painting Fool is a case study. This, along with a philosophical
underpinning given in (Colton et al. 2014) provide a general grounding for the design decisions presented here, in
terms of why they represent significant progress towards the
long-term aim of public acceptance of The Painting Fool as a
creative artist. In this context, we describe here three new art
projects where The Painting Fool has used its new visual capabilities with increasing sophistication, to produce interesting art and experiences for audiences via more autonomous
behaviours in the software. These projects include a moodbased portraiture demonstration, where the visual processing was used to express intent; The Painting Fool’s first art
commission for a third party; and a private art project.
The collaborative projects with DARCI have progressed
The Painting Fool project along a number of axes. With machine vision abilities, it can now analyse its output, albeit
simplistically: new functionality with potential to make it
more appreciative in motivating and assessing projects, and
via analysis during sketching activities. Also, choosing rendering styles can now be done by the software itself, rather
than a person. This adds much autonomy, increases impressions of creative responsibility in the software, and has led to
surprising results, as the paintings no longer only resemble
those produced in traditional ways by people. The Painting
Fool uses the digital medium more fully in interesting new
styles difficult for people to achieve, which again increases
the impression of independence and creative responsibility.
This paper is organised as follows. In the next section,
we describe aspects of The Painting Fool and DARCI used
in the collaboration, followed by a discussion of how association networks from DARCI were used by The Painting Fool in increasing levels of sophistication. We then
present the three new art projects enabled by this collaboration, and put these into the context of related work. We conclude with a discussion of the advances made in The Painting Fool project, and we briefly question whether software
artists need to see in the same way as people.

The segmentation and rendering methods are highly parameterised, requiring 14 and 57 parameters to be set respectively, as described in (Colton 2012b). Choosing from
the space of possible segmentation and rendering methods
constitutes a large part of the creative responsibility taken
on in an art project, along with choosing and arranging subject matter, etc. We show below how the software now takes
on the responsibility of choosing the rendering settings.

DARCI: Association Networks

Figure 1: A workflow representation of The Painting Fool’s
processing for the You Can’t Know My Mind exhibit.

Background
The Painting Fool: Workflows
There is no single way in which The Painting Fool produces
artworks, but rather a set of tasks it can achieve through
performing certain behaviours, and workflows which combine these into art-producing processes. The behaviours
make use of various AI techniques including natural language processing (Krzeczkowska et al. 2010), constraint
solving (Colton 2008b), evolutionary search (Colton 2008a),
design grammars (Colton and P´erez-Ferrer 2012) and machine learning (Colton 2012a). The workflows are constructed through a teaching interface currently consisting of
24 screens. An example workflow, for the You Can’t Know
My Mind exhibit (described below) is given in figure 1. This
highlights that the vision system is used both at the start of
the process and towards the end (the ‘AN evaluation’ node).
Before the work described here, The Painting Fool had
a very rudimentary visual analysis system that was able to
evaluate features of an image such as texture, colour variance and symmetry. It is also able to segment a given digital
photograph into a set of colour regions, using a thresholdbased neighbourhood construction method, path-finding for
edge rationalisation and edge abstraction methods. A waypoint in every workflow is the construction of such a set of
colour regions, which can be achieved using this segmentation process, via design grammars, variation of hand-drawn
scenes and/or a constraint solver placing rectangles onto the
canvas. The colour regions direct the rendering process,
whereby each region is either filled-in or outlined via the
simulation of natural media such as paints and implements
such as paintbrushes. The rendering of each region can include multiple fill/outline passes, and the rendering of the
entire segmentation of colour regions can be done repeatedly, building up a layered image.

One way for The Painting Fool to have an increased appreciation of the artefacts it produces and some level of intentionality (both desirable qualities), is for it to employ a perceptually grounded cognitive model that can associate visual
stimuli with linguistic concepts. That ability was realized by
borrowing a piece of the DARCI system, a visuo-linguistic
association approach, which consists of a set of neural networks that perform a mapping from low-level computer vision features to adjectival linguistic concepts, learned from
a corpus of human-labeled images.
These images come from a continuously growing dataset
obtained via a public facing website (darci.cs.byu.edu)
that solicits volunteer labeling of random images. Volunteers are allowed to label images with any and all adjectives
they think describe the image, and as a result, images can be
described by their emotional effects, most of their aesthetic
qualities, many of their possible associations and meanings,
and even, to some extent, by their subject. Furthermore,
through additional labeling exercises, volunteers can specify labels that explicitly do not describe the image, allowing
the collection of explicit negative labels as well as positive
ones. The result is a rich, challenging, dynamic dataset. A
recent snapshot of the data reveals 17,004 positive labels and
16,125 negative labels using 2,463 unique adjectives associated with 2,562 unique images, an average of approximately
12 unique labels per image, and 110 adjectives with at least
30 positive and 30 negative image associations.
Images are perceived by the system as a vector of 102
low-level computer vision features extracted from the image using the DISCOVIR system1 . This level of image perception does not admit significant semantic understanding,
but it does allow appreciation of concepts that can be adequately expressed with global, abstract features dealing with
characteristics of the image’s color, lighting, texture, and
shape. Given training data in the form of (image feature
vector, adjectival label) pairs, a mapping is learned using
a set of artificial neural networks that we call association
networks. Since learning image-to-concept associations is
a multi-label classification problem, and we cannot assume
implicit negativity, the only appreciation networks trained
for a particular image are those explicitly labeled with (positive or negative examples of) the associated concept. Each
adjectival concept is learned by a unique association network, which is trained using standard backpropagation and
outputs a single real value, between 0 and 1, indicating the
degree to which an input image can be described by the network’s associated adjectival concept.
1

appsrv.cse.cuhk.edu.hk/˜miplab/discovir

Figure 2: Seventeen painting styles along with layering scheme and partial visual profile.

Implementing Vision-Enhanced Painting
From the DARCI system, The Painting Fool inherited a set
of 236 association networks (ANs), and a method of turning
a given image I into the numerical inputs to the ANs. Each
AN corresponds to a particular adjective, i.e., the higher the
output from the AN for adjective A when given input values
for I, the more likely (the AN predicts) that a viewer will use
A to describe I. We first determined which of the adjectival
ANs were suitable for dealing with The Painting Fool’s output. To do this, we ran each AN over hundreds of painterly
images from The Painting Fool and recorded the range of
the numerical outputs. We found that for the majority of the
ANs, the output range was so low that we couldn’t meaningfully claim that it was differentiating between images based
on visual properties. We selected all ANs where the range
of outputs was 0.05 or greater, and then performed a sanity
check on those remaining, removing any which described
images in a particularly counter-intuitive way, e.g., the AN
for ‘red’ outputting a higher score for a patently green image
than for a patently red image.
This left a selection of 65 usable ANs, to which we implemented an interface in The Painting Fool. For each selected
AN, we recorded the highest and lowest outputs over the
hundreds of images mentioned above, and when output from
a new image is calculated, it is normalised between these
extremes. As described below, the ANs have been used in a
number of new workflow behaviours for The Painting Fool.
The simplest of these is to allow the software to frame it’s
output (Charnley, Pease, and Colton 2012) by describing it
visually. It can also compare and contrast images in terms
of a particular adjective, or in terms of a profile of multiple
adjectives. It can also employ the ANs during the painting
process, as described in the following subsections.

A Space of Simulated Visual Art Implements/Styles
Given an image segmentation of colour regions as described
above, The Painting Fool produces a non-photorealistic rendering of it in a series of whole-segmentation layers, during
which each region itself is rendered in multiple layers. During the rendering of each layer, which can either be filled
in, or outlined, the software simulates natural media such
as paints, and the usage of implements such as brushes in
outline/fill styles such as hatching, as described in (Colton
2012b). The rendering of a layer is determined by a set of
57 parameters, which cover the simulation of the media itself (e.g., wetness of paint), the implement (e.g., brush size)
the support (e.g., canvas roughness) and the style (e.g., number of times to draw an outline).

We defined a space of painting styles by fixing the rendering to a single whole-segmentation layer during which
a scheme of up to five rendering layers per region was allowed. The region layering scheme was represented as a
string with letters A, B, C, a, b or c. Upper case letters represent a fill layer with lower case letters representing outline
layers. Where upper and lower case letters correspond (e.g.,
A = a), all the other settings are the same, hence they represent the simulation of the same natural media in roughly
the same way, but one produces an outline, the other produces a fill. For instance, ABCab represents three fill layers
and two outline layers, with all the settings of the first two
fill layers exactly as for the two outline layers. We found
that it increased visual coherence if the fill layers corresponded to the outline layers in this way. After initial experimentation, we constrained the space to include five layering schemes: aB, Ba, ABab, Aab and ABCab, which we
found produced a suitably large variety of visual styles.
We generated 1,200 painting styles by randomly sampling
the space of rendering styles with each of the 57 parameters set randomly to an appropriate value in its range, and
then mapping a set of these styles onto one of the five layering styles above, also chosen randomly. For each style, we
used The Painting Fool to render a given segmentation of
an abstract flower. Then, for each of the 65 selected ANs
described above, we calculated the normalised output for
each of the 1,200 flower paintings, thus creating a visual
profile for each style. Example painting styles, along with
the layering scheme and part of their visual profile are given
in figure 2. The seventeen pictures demonstrate somewhat
the diversity in the painting styles within this space. The
partial profiles indicate that while the AN outputs have a relatively small range, it is sufficient for a choice of painting
style based on these values to be meaningful.

Employing Vision During Painting
To recap, we supplied The Painting Fool with 1,200 different painting styles, each with a visual profile derived from
applying association networks. As described above, there
are various workflows for producing images with The Painting Fool. When the workflow starts with a digital photograph, images are segmented into a certain number of colour
regions, with more regions usually leading to more photorealism in the final paintings. Each colour region corresponds, therefore, to a region of the original photograph, and
this photo-region can be interrogated in order to choose an
appropriate painting style. To do this, The Painting Fool extracts the photo region onto a transparent image, then applies

all 65 adjectival ANs to the extract, to compile a profile. The
Euclidean distance of this photo-extract profile from the visual profile of the 1,200 painting styles is used to order the
styles in increasing distance. The distance can be interpreted
as an appropriateness of the painting style to the underlying
photo extract. That is, the style with least distance will render the region in a way that is most similar in nature to the
original photograph (according to the ANs).
The new workflow for The Painting Fool uses machine
vision during painting as follows: it takes a photograph and
segments it into colour regions. For each colour region, a
photo-extract profile is produced using the ANs, and this
is used to order the painting styles in The Painting Fool’s
database, in terms of how appropriate they are to the photo
extract. From the top ten most appropriate styles, one is
chosen randomly to paint the region in question. Choosing
from the top ten in this fashion means that each time the
same photograph is painted, it produces a different image,
yet each time, each painting style is appropriate to the region it is used to paint. We have enhanced this workflow by
enabling a sketching mechanism. That is, The Painting Fool
tries all of the ten most appropriate sketching styles in situ,
then produces a visual profile of the resulting region of the
painting, and chooses the one where this profile is closest
to the photo-region profile. This reduces the reliance on the
initial flower experiments somewhat, as The Painting Fool
can see what each style looks like actually in the painting,
before committing to one in particular. It also opens up the
potential for The Painting Fool to produce a sketchbook to
accompany each painting, as framing information.

Cultural Applications
In the subsections below, we describe new cultural application projects with The Painting Fool which have been enabled by its access to a vision system. These span the kinds
of public, private and commissioned art projects that an artist
might expect to undertake as part of their general activities.

The ‘You Can’t Know My Mind’ Exhibition
For the You Can’t Know My Mind exhibit reported in (Colton
and Ventura 2014), we focused on the question of intentionality in creative software. As software is programmed directly, it is fair criticism to highlight that in most Computational Creativity projects, the intention for the production of
artefacts comes from the software’s author and/or user. For
the You Can’t Know My Mind project, we raised our intentions to the meta-level, i.e., we intended for the software to
produce portraits and entertain sitters in order to learn about
its own painting styles. However, the aim of each artefact
production session was determined by The Painting Fool itself, in order for it to exhibit behaviours that unbiased observers might project the word ‘intentionality’ onto.
An 8-point description of how The Painting Fool operated
in this project is given in (Colton and Ventura 2014). Of
note here, we used the machine vision system from DARCI
offline, to prepare the software for portraiture sessions. That
is, for each of 1,000 abstract art images produced by the
Elvira sub-module (Colton, Cook, and Raad 2011), and for

Figure 3: Example comparisons of sketch conceptions (left)
and associated portraits (right). The percentages portray the
range of the output from the adjective AN for the second
image in terms of the AN output value for the first image.
each of 1,000 image filters produced by the Filter Feast submodule (Torres, Colton, and Rueger 2008), the output of all
of the adjective ANs were calculated. Hence the software
could choose from the most appropriate abstract backdrops
and the most appropriate filters for an adjective, A, chosen
to fit a mood, to produce a sketch conception to aim for with
each portrait. The ‘background image’ and ‘filtered image
conception’ nodes in figure 1 correspond to these.
Under the assumption that the sketch will invoke people
to project certain adjectives onto the image upon viewing,
the sketch conception has aspects which The Painting Fool
aspires to achieve in its painting. The conception image is
segmented into colour regions, and a simulation of various
painting media (paints, pastels and pencils) are used in one
of eight styles, to produce a portrait. At the end of each portraiture session, The Painting Fool uses the vision system to
compare the level of adjective projection in the portrait to
that of the sketch. To do this (indicated by the ‘AN evaluation’ node in figure 1), it applies the adjectival AN for A
to the sketch conception and to the final portrait, and compares the output. If the AN output for the portrait, Op , is
within 95% to 105% of the AN output for the conception,
Oc , i.e., 0.95 × Oc ≤ Op ≤ 1.05 × Oc , this is recorded as
satisfactory. If it is higher than 105%, this is recorded as a
success, and if it is higher than 110%, this is recorded as a
great achievement, with failures similarly recorded. Three
examples comparisons of conception and portrait are given
in figure 3. The level of achievement/failure is used to update a probability distribution that The Painting Fool can use
to choose painting styles later to (attempt to) achieve an image with maximal output respect to a given adjectival AN.

Figure 4: Front page and excerpt from the Japanese version of the essay for the ‘I Can See Unclearly Now’ commission. Third
image: an early photograph of artwork hung in the Behaviour Informatics Laboratories of UBIC.

The ‘I Can See Unclearly Now’ Commission
2

UBIC is a behavioural information data analysis company
based in Tokyo. In early August of 2014, UBIC’s CTO,
Mr. Hideki Takeda came across The Painting Fool’s website
while exploring recent advances in Artificial Intelligence research on the web. At that time, UBIC’s Behavior Informatics Laboratories (BIL) in Shinagawa, Tokyo, was implementing a complete office renovation scheme reflecting
the company’s reorientation from eDiscovery vendor to supplier of in-house Big Data Analytics solutions powered by
an AI engine called the Virtual Data Scientist. The new office concept of the BIL can be summed up as: “Shaking the
boundaries between the virtual and the real so as to stimulate the senses and promote intelligence and creativity”. For
example, the new office features both real bamboo and bamboo imprinted on a glass wall. The choice of bamboo is not
arbitrary, but motivated by the fact that this plant plays a
prominent role in traditional Japanese culture. It is highly
symbolic and associated with, for example, Noh theatre3 in
which the protagonists are often ghosts from another plane
of existence but appearing in the real world.
Mr. Takeda decided to commission artworks from The
Painting Fool, as this would fit very well with the blurring of
virtual and real spaces in the BIL The first author of this paper – who is the lead researcher in The Painting Fool project
– was contacted by the second author acting on behalf of
UBIC, and ultimately three series of images were commissioned, along with an essay highlighting how the machine
vision system was used in increasingly sophisticated ways
from the first to the third series. Constraints were put on the
commission: (i) to include a portrait from a live sitting, and
(ii) to include a piece involving Alan Turing, as an AI pioneer. Moreover, it was agreed that the commission would
involve an element of research and implementation, driving
The Painting Fool project forward. Example images (with
details) from the three series are given in figure 6, and details from the essay, along with an early photograph of one
of the pieces hung in the BIL are given in figure 4. The title of the commission was chosen to highlight The Painting
Fool’s new usage of machine vision techniques, while indicating that the system is far from perfect.
To tie the three series of images together, the same style of
2
3

www.ubicna.com
en.wikipedia.org/wiki/Noh

backdrop was used, consisting of 10,000 adjectives rendered
in a handwritten way in varying shades of greyscale pencil,
onto dark backgrounds. In all the pieces, the mass of adjectives open up in multiple places into which red handwritten adjectives are strategically placed. For the first series,
StarFlowers, paintings of the abstract flowers used for assessing painting styles were placed using a constraint solver
to avoid overlap, as per (Colton 2008b), with slightly differing sizes. Before placement, each flower image was assessed
by the 65 adjective ANs, and from the top ten highest scoring
adjectives, two were chosen to appear alongside the flower
in the piece, in red handwriting. The pairs were automatically chosen so that no flower had the same two adjectives
next to it. For instance, in the detail of figure 6, the first
flower is annotated with ‘peaceful’ and ‘warm’.
In the second series, Good Day, Bad Day, two photographs of the second author seated, posing firstly in a
good mood, and secondly in a bad mood were used. The
65 adjectives were split into positive, neutral and negative
valence categories, e.g., happy, glazed, bleary respectively.
The painting style with the highest average AN output over
the positive adjectives was chosen to paint the first pose,
and the most negative style was similarly chosen to paint the
second pose. Each portrait was annotated at its edges with
red handwritten adjectives appropriate to the painting at that
edge point. In the third series, Dynamic Portraits: Alan Turing, a photograph of Turing was hand annotated with lines
to pick out his features. We then used the method of arbitrarily choosing from the top ten most appropriate painting
styles for each colour region described above, to produce a
number of portraits, with the annotated lines being painted
on at the end, to gain a likeness. The rendered painting was
analysed with the 65 ANs and the 17 most appropriate adjectives were scattered around the backdrop of the image, in
a non-overlapping way, as usual in red handwriting.
Dozens of images from the three series were sent to UBIC
to choose from for the BIL, with very little curation from the
first author. UBIC representatives confirmed that the commission achieved the brief of producing pieces which blur
the line between the real (i.e., painted by a person) and the
virtual (i.e., painted by a computer), and were very happy
with the commission. They produced a translated version of
the essay for visitors to the lab, and hung an example from
each series in the BIL.

Related Work

Wired and PC Gamer came much later, when ANGELINA
had more independence and could produce full games, given
just an initial theme of a short phrase, proposed by the journalist. For the PC Gamer game, NBA Mesquite Volume 2,
ANGELINA used a database of labelled textures compiled
from social media mining, for the first time in a released
game. This happened because the theme chosen, ‘avocado’,
matched a label in the database for the first time since the
database had been added. This created an additional talking point for the article, and in general the games were well
received and drove up online viewing figures.
The Paul drawing robot by Patrick Tresset (Tresset and
Fol Leymarie 2012) has much in common with The Painting
Fool, in that it uses a camera and machine vision techniques
to capture an image, then automatically draws a portrait: in
this case, physically, using a robotic arm and a pen. It also
simulates looking while it draws, but this is only for entertainment purposes, i.e., after the initial photograph is taken,
the vision system is not used again. Paul has been commissioned on a number of occasions, most notably for a weeklong workshop at the Centre Pompidou in late 2013. Tresset has also found success in selling versions of the robot
painter to art museums. Another robotic painter, which does
use machine vision during painting and has also been commissioned for art is the eDavid system, as described by (Lindemeier, Pirk, and Deussen 2013). Here, a camera is used
to photograph the canvas after a series of paint strokes have
been applied, with a vision system employed to optimise the
placement of future strokes based on the visual feedback.
It is beyond the scope of this paper to perform a survey of commissions where software creators rather than
artists controlling software have produced artworks. However, we can tentatively introduce some metrics for comparing projects/software/programmers to begin to characterise
such commissions. For instance, one could compare the domain specific training of the programmer, e.g., comparing
the commissions of artist Harold Cohen (who represented
the UK in the Venice Biennale) and his AARON system
(McCorduck 1991) with Oliver Deussen (who has no artistic training) and his eDavid system mentioned above, as this
may indicate more autonomy in the software (but doesn’t
necessarily). Other measures could include how much curation takes place, i.e., how much of the software’s output is
usable; what amount of hand-finishing of output takes place;
and how much extra coding is required for each project.

It is commonplace for an artist to be commissioned to work
with a bespoke piece of software, or even to develop new
code, to produce artwork, with the person using the software
as a tool, and this tool may be generative. However, it is
much less common for a commission to be made specifically
because the software will take on some of the creative, not
merely generative, responsibilities.
The ANGELINA system (Cook and Colton 2014) has
been commissioned to produce games for the New Scientist, Wired and PC Gamer Magazines. In the former, ANGELINA designed a game as normal, but its designer provided custom visual theming, drawing new sprites and creating sound effects for Space Station Invaders, since ANGELINA was not capable of this. The commissions for

Through the above projects, The Painting Fool has advanced
as an artist in three major ways. Firstly, the creative responsibility of choosing a painting style has been handed to the
software. With the You Can’t Know My Mind project, it
learned a probability distribution which can choose between
one of eight painterly rendering styles, to produce an image which people will probably describe using an adjective,
chosen intentionally to express a mood. With the I Can See
Unclearly Now project, the software gained the ability to
choose between 1,200 painting styles for each colour region
dynamically during painting. With the Portrait of Geraint
Wiggins project, it went further: performing in situ sketches

Figure 5: Portrait of Geraint Wiggins.

The Portrait of Geraint Wiggins
In (rather belated) celebration of a milestone birthday, we
used the vision-based sketching approach described above,
to produce a portrait. Given an original image, handannotated with lines picking out facial features, The Painting Fool segmented it into 150 colour regions/lines, and for
each, chose the top ten most appropriate painting styles,
as described above. For each of the ten, it painted the region, calculated the visual profile of the region of the painting that resulted, and finally chose the style with minimal
distance between its visual profile and that of the original
photo-extract. In this way, the painting process was deterministic, but not predictable, and produced a striking portrait
with painterly and distinctly non-painterly effects. To add a
physical uniqueness, the image was printed onto 300 4cm
by 4cm squares which were composed into the final piece
in an overlapping formation, as per the Dancing Salesman
Problem piece described in (Colton and P´erez-Ferrer 2012).
The portrait is shown in figure 5.

Conclusions and Future Work

to test painting styles in the context of the painting at hand.
Hence, the decision making involved in determining rendering styles is now undertaken by the software, which is a major advance in autonomy, and potentially towards its acceptance as an artist in its own right.
Secondly, on close inspection of the pieces in figures 5
and 6, while the images produced retain a painterly style
somewhat, there are aspects which couldn’t be produced
with natural media simulation. This is because the painting styles in its database include ones which simulate the
ground in-between natural media such as paints and pastels,
and others which have no analogue in the physical world.
This means that – for the first time – The Painting Fool can
produce images using a much broader range of pixel manipulations, producing styles which have little grounding in
traditional painting, We also see this as a major advance, as
it extends the variety of images the software can produce,
and potentially increases perceptions of autonomy.
The third advance will be expressed more in future work
than in the projects presented here. Through the mapping
of visual stimuli to linguistic concepts, The Painting Fool is
able to project adjectives onto images, and we plan to enhance this with the ability to similarly project nouns. This
will increase its capacity to appreciate its own work and that
of others, enabling it to provide more sophisticated commentaries about what it has produced, and we touched on this
with the output in the You Can’t Know My Mind project,
where the conceived and rendered images are compared
visually. We plan to take this framing further, with The
Painting Fool keeping a sketchbook for each project, adding
value, and helping audiences to understand its processes.
It is clear from figure 2 that the visio-linguistic system
does not yet match that of people perfectly. Moreover, we
acknowledge that – as pointed out by a reviewer – we have
not provided data to verify that our strategy to match the visual profile of an image with appropriate painting styles for
regions is a good strategy, nor have we yet compared and
contrasted alternatives or tested people’s reactions to the artworks produced. We aim to experiment with this approach
and explore alternatives in future work. However, before undertaking much further work, we wish to raise, discuss and
be guided by responses to a philosophical question for the
Computational Creativity community: is it important that an
automated artist has a visual system similar to that of people? For communication/framing value, it might be preferable for the software’s visual judgements to match ours
closely. However, as illustrated by a recent internet storm
about colours in a dress (Rogers 2015), we all have different visual perception systems, and notions of beauty differ
from generation to generation and person to person. As art
is driven forward by such differences, it may be more interesting and important artistically for us to learn The Painting
Fool’s visual system, rather than it learning ours.

Acknowledgements
We wish to thank the anonymous reviewers for their very
helpful and insightful comments. This work was funded by
EPSRC project EP/J004049, EC funding from the FP7 ERA
Chair scheme (project number 621403) and by UBIC Inc.

References
Charnley, J.; Pease, A.; and Colton, S. 2012. On the notion of
framing in computational creativity. Proc. 3rd ICCC.
Colton, S., and P´erez-Ferrer, B. 2012. No photos harmed/growing
paths from seed – an exhibition.
In Proceedings of NonPhotorealistic Animation and Rendering.
Colton, S.; Cook, M.; Hepworth, R.; and Pease, A. 2014. On Acid
Drops and Teardrops: Observer Issues in Computational Creativity.
In Proceedings of the AISB symposium on AI and Philosophy.
Colton, S., and Ventura, D. 2014. You Can’t Know My Mind: A
festival of computational creativity. Proc. 5th ICCC.
Colton, S.; Pease, A.; Corneli, J.; Cook, M.; Hepworth, R.; and
Ventura, D. 2015. Stakeholder groups in computational creativity research and practice. In Besold, T.; Schorlemmer, M.; and
Smaill, A., eds., Computational Creativity Research: Towards Creative Machines. Springer.
Colton, S.; Cook, M.; and Raad, A. 2011. Ludic considerations of
tablet-based evo-art. In Proceedings of EvoMusArt.
Colton, S. 2008a. Automatic invention of fitness functions with
application to scene generation. In Proceedings of EvoMusArt.
Colton, S. 2008b. Experiments in constraint based automated scene
generation. In Proceedings of the fifth international workshop on
Computational Creativity.
Colton, S. 2012a. Evolving a library of artistic scene descriptors.
In Proceedings of EvoMusArt.
Colton, S. 2012b. The Painting Fool: Stories from building an
automated painter. In McCormack, J., and d’Inverno, M., eds.,
Computers and Creativity, 3–38. Springer.
Cook, M., and Colton, S. 2014. Ludus ex machina: Building a 3D
game designer that competes alongside humans. Proc. 5th ICCC.
Heath, D.; Norton, D.; and Ventura, D. 2014. Conveying semantics
through visual metaphor. ACM Transactions on Intelligent Systems
and Technology 5:31.
Krzeczkowska, A.; El-Hage, J.; Colton, S.; and Clark, S. 2010.
Automated collage generation – with intent. Proc. 1st ICCC.
Lindemeier, T.; Pirk, S.; and Deussen, O. 2013. Image stylization with a painting machine using semantic hints. Computers and
Graphics 37(5):293–301.
McCorduck, P. 1991. AARON’s Code: Meta-Art, Artificial Intelligence, and the Work of Harold Cohen. W. H. Freeman & Co.
Montfort, N.; P´erez y P´erez, R.; Harrell, F.; and Campana, A. 2013.
Slant: A blackboard system to generate plot, figuration, and narrative discourse aspects of stories. Proc. 4th ICCC.
Norton, D.; Heath, D.; and Ventura, D. 2010. Establishing appreciation in a creative system. Proc. 1st ICCC.
Norton, D.; Heath, D.; and Ventura, D. 2011. Autonomously creating quality images. Proc. 2nd ICCC.
Norton, D.; Heath, D.; and Ventura, D. 2013. Finding creativity in
an artificial artist. Journal of Creative Behavior 47(2).
Rogers, A. 2015. The science of why no one agrees on the colour
of this dress. Wired, Science Section, 26th Feb.
Strothotte, T., and Schlechtweg, S. 2002. Non-Photorealistic Computer Graphics. Morgan Kaufmann.
Torres, P.; Colton, S.; and Rueger, S. 2008. Experiments in example based image filter retrieval. In Proceedings of the Workshop on
Cross-Media Information Analysis, Extraction and Management.
Tresset, P., and Fol Leymarie, F. 2012. Sketches by Paul the robot.
In Proceedings of the 8th Annual Symposium on Computational
Aesthetics in Graphics, Visualization, and Imaging.

Figure 6: Example images, each with detail, from the ‘I Can See Unclearly Now’ commission. First pair: from the Star Flowers
series. Second pair: from the Good Day, Bad Day series. Third pair: from the Dynamic Portraits: Alan Turing series.

